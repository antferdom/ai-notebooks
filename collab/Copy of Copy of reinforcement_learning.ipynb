{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Copy of reinforcement_learning.ipynb","provenance":[{"file_id":"https://github.com/henrywoo/MyML/blob/master/Copy_of_reinforcement_learning.ipynb","timestamp":1652808541033}],"collapsed_sections":["qv0Xp2ZHDOq7"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"metadata":{"id":"qv0Xp2ZHDOq7"},"cell_type":"markdown","source":["#### Copyright 2018 Google LLC."]},{"metadata":{"id":"Ja8W3KL-DQ4j"},"cell_type":"code","source":["# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"metadata":{"id":"qc-lJM2jDTRr"},"cell_type":"markdown","source":["# Reinforcement Learning\n","\n","In this lab, we define an agent and environment using [OpenAI Gym](https://gym.openai.com/), a toolkit for developing reinforcement learning algorithms, which is compatible with TensorFlow. Gym provides us with a collection of [environments](https://gym.openai.com/docs/#environments), such as:\n","\n","*   [Classic control](https://gym.openai.com/envs/#classic_control): control theory problems from the classic RL literature\n","*   [Toy text](https://gym.openai.com/envs/#toy_text): simple text environments\n","*   [Algorithms](https://gym.openai.com/envs/#algorithmic): learn to imitate computations\n","*   [Atari](https://gym.openai.com/envs/#atari): Atari 2600 games\n","*   [Box2D](https://gym.openai.com/envs/#box2d): continuous control tasks in the Box2D simulator\n","*   [Robotics](https://gym.openai.com/envs/#robotics): simulated goal-based tasks for the Fetch and ShadowHand robots\n","*   [MuJoCo](https://gym.openai.com/envs/#mujoco): continuous control tasks, running in a fast physics simulator\n","\n","We start by installing some libraries, then define the agent and environment, train an agent on CartPole, and one on Atari.\n","\n","## Outline\n","  1. Setup\n","  1. Episodes\n","  1. Summaries\n","  1. Observations\n","  1. Agent\n","  1. Experiment\n","  1. Task 1: Training an agent on CartPole\n","  1. Task 2: Training an agent on Atari\n","\n","\n","Please **make a copy** of this Colab notebook before starting this lab. To do so, choose **File**->**Save a copy in Drive**."]},{"metadata":{"id":"w6ZWy5D7DfGE"},"cell_type":"markdown","source":["## Setup\n","\n","Let's get started by installing some libraries."]},{"metadata":{"id":"fSKX8cdrBiTs","cellView":"both","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652807318854,"user_tz":-120,"elapsed":19652,"user":{"displayName":"","userId":""}},"outputId":"bafddbe7-852d-4750-fe6f-8ef0825554f2"},"cell_type":"code","source":["# for open ai gym\n","!pip install -q gym\n","!apt -qq install cmake\n","!pip install \"gym[atari, classic_control]\"\n","# !apt-get install python-opengl"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n","The following packages were automatically installed and are no longer required:\n","  libnvidia-common-460 nsight-compute-2020.2.0\n","Use 'apt autoremove' to remove them.\n","0 upgraded, 0 newly installed, 0 to remove and 67 not upgraded.\n","Requirement already satisfied: gym[atari,classic_control] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]) (1.21.6)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]) (1.5.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]) (1.4.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]) (7.1.2)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]) (4.1.2.30)\n","Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]) (0.2.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]) (1.15.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]) (0.16.0)\n","Collecting git+https://github.com/jakevdp/JSAnimation.git\n","  Cloning https://github.com/jakevdp/JSAnimation.git to /tmp/pip-req-build-xilfjmr5\n","  Running command git clone -q https://github.com/jakevdp/JSAnimation.git /tmp/pip-req-build-xilfjmr5\n"]}]},{"cell_type":"code","source":["# for viualization\n","!pip install git+https://github.com/jakevdp/JSAnimation.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"upHaUPkpEeLU","executionInfo":{"status":"ok","timestamp":1652808620904,"user_tz":-120,"elapsed":12252,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}},"outputId":"67df178e-0f6d-4a66-ae53-fbb52faee102"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/jakevdp/JSAnimation.git\n","  Cloning https://github.com/jakevdp/JSAnimation.git to /tmp/pip-req-build-abs02qr6\n","  Running command git clone -q https://github.com/jakevdp/JSAnimation.git /tmp/pip-req-build-abs02qr6\n","Building wheels for collected packages: JSAnimation\n","  Building wheel for JSAnimation (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for JSAnimation: filename=JSAnimation-0.1-py3-none-any.whl size=12295 sha256=1eeb00ca21b0916480dcfac15f102582b4d1d7df0603c7abcf6a412421ae76a9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-1ny8jrrp/wheels/2e/69/c1/62e849a90a158df5c1d33ed530e381a74589f2be56d699c156\n","Successfully built JSAnimation\n","Installing collected packages: JSAnimation\n","Successfully installed JSAnimation-0.1\n"]}]},{"metadata":{"id":"pe8k9LxdOsEt"},"cell_type":"markdown","source":["We then load a helper function for visualizations."]},{"metadata":{"id":"9Znp38vU-_8k","cellView":"both","executionInfo":{"status":"ok","timestamp":1652809074900,"user_tz":-120,"elapsed":209,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["# Source: https://github.com/jakevdp/JSAnimation\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","from IPython.display import display\n","\n","\n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    display(display_animation(anim, default_mode='loop'))"],"execution_count":1,"outputs":[]},{"metadata":{"id":"Czsll0jyOxAm"},"cell_type":"markdown","source":["Finally, run the following code cell to import the necessary libraries."]},{"metadata":{"id":"IbEy-u2MGxhy","cellView":"both","executionInfo":{"status":"ok","timestamp":1652809092379,"user_tz":-120,"elapsed":6889,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["import collections\n","import gym\n","import numpy as np\n","import tensorflow as tf\n","import time"],"execution_count":2,"outputs":[]},{"metadata":{"id":"Se3PrgWSO25X"},"cell_type":"markdown","source":["We're now ready to start defining the following:\n","\n","*   Episodes\n","*   Summaries\n","*   Observations\n","*   The agent\n","*   The experiment"]},{"metadata":{"id":"MXdmtiNy2Ddj"},"cell_type":"markdown","source":["## Episodes\n","\n","We start by defining episodes, which are made up of:\n","\n","*   actions\n","*   observations\n","*   rewards\n","*   length"]},{"metadata":{"id":"NNPiQQn-Gu2N","executionInfo":{"status":"ok","timestamp":1652808637686,"user_tz":-120,"elapsed":356,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["Episode = collections.namedtuple(\"Episode\", [\"actions\", \"observations\", \"rewards\", \"length\"])"],"execution_count":5,"outputs":[]},{"metadata":{"id":"xRZ-jZZMS2et"},"cell_type":"markdown","source":["The following function which creates a collection of episodes will be used when we define an experiment."]},{"metadata":{"id":"Xn3q3989j0BR","executionInfo":{"status":"ok","timestamp":1652808641787,"user_tz":-120,"elapsed":201,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["def collect_n_episodes(num_ep, max_ep_len, env, agent, session, animate=False):\n","    episodes = []\n","    frames = []\n","    for ep in range(num_ep):\n","        obs = env.reset()\n","        observations, actions, rewards = [], [], []\n","\n","        for step in range(int(max_ep_len)):\n","            observations.append(obs)\n","            action = session.run(agent.sampled_action, feed_dict={agent.obs_ph: obs[None]})\n","            # squeeze the batch dimension, hence action[0]\n","            action = action[0]\n","            obs, reward, done, _ = env.step(action)\n","            if animate:\n","              frames.append(env.render(mode='rgb_array'))          \n","            actions.append(action)\n","            rewards.append(reward)\n","            if done:\n","                break\n","        episodes.append(Episode(actions=actions, observations=observations, rewards=rewards, length=len(actions)))\n","    \n","    if animate:\n","      return frames\n","    \n","    return episodes"],"execution_count":6,"outputs":[]},{"metadata":{"id":"Z0ktrQ6SkECC"},"cell_type":"markdown","source":["## Summaries\n","\n","We then define and compute summaries,  which are made up of:\n","\n","*   time elapsed\n","*   number of global steps\n","*   policy loss\n","*   mean\n","*   standard deviation\n","*   min and max return\n","*   mean and standard deviation for episode length"]},{"metadata":{"id":"XkGfxf2v3gip","executionInfo":{"status":"ok","timestamp":1652808644665,"user_tz":-120,"elapsed":2,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["Summary = collections.namedtuple(\"Summary\",\n","                                 [\"time_elapsed\", \"global_steps\", \"mean_return\", \"std_return\", \"min_return\",\n","                                  \"max_return\", \"mean_episode_len\",\n","                                  \"std_episode_len\", \"policy_loss\"])"],"execution_count":7,"outputs":[]},{"metadata":{"id":"92w42337S_ql"},"cell_type":"markdown","source":["The following function will be called to create a summary when we run an experiment."]},{"metadata":{"id":"tFHLLSMnkFay","executionInfo":{"status":"ok","timestamp":1652808646491,"user_tz":-120,"elapsed":273,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["def compute_summary(episodes, time_elapsed, policy_loss, global_steps):\n","    returns = [np.sum(e.rewards) for e in episodes]\n","    ep_lengths = [e.length for e in episodes]\n","    return Summary(time_elapsed=time_elapsed, global_steps=global_steps, policy_loss=policy_loss,\n","                   mean_return=np.mean(returns),\n","                   std_return=np.std(returns), max_return=np.max(returns), min_return=np.min(returns),\n","                   mean_episode_len=np.mean(ep_lengths), std_episode_len=np.std(ep_lengths))\n"],"execution_count":8,"outputs":[]},{"metadata":{"id":"KjJPwXaTkHR8"},"cell_type":"markdown","source":["## Observations\n","\n","We then define a function to encode observations."]},{"metadata":{"id":"LrPE_mimkJDm","executionInfo":{"status":"ok","timestamp":1652808648862,"user_tz":-120,"elapsed":4,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["def encode_observation(obs_ph, output_size, scope, n_layers=2, size=32, activation=tf.tanh, output_activation=None):\n","    with tf.compat.v1.variable_scope(scope):\n","        dense = obs_ph\n","        for i in range(n_layers):\n","            dense = tf.compat.v1.layers.dense(inputs=dense, units=size, activation=activation)\n","        return tf.compat.v1.layers.dense(inputs=dense, units=output_size, activation=output_activation)"],"execution_count":9,"outputs":[]},{"metadata":{"id":"Pz_3keWtkLmS"},"cell_type":"markdown","source":["## The agent\n","\n","We now define the Actor-Critic agent."]},{"metadata":{"id":"rtUd4kMW3lRX","executionInfo":{"status":"ok","timestamp":1652808657173,"user_tz":-120,"elapsed":199,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["A2cAgent = collections.namedtuple(\"A2cAgent\",\n","                                  [\"obs_ph\", \"action_ph\", \"advantage_ph\", \"value_ph\", \"sampled_action\",\n","                                   \"value_prediction\", \"critic_train_op\", \"policy_loss\", \"policy_train_op\"])"],"execution_count":10,"outputs":[]},{"metadata":{"id":"-FteEoPf2A95","executionInfo":{"status":"ok","timestamp":1652808679264,"user_tz":-120,"elapsed":387,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["def build_network(env, learning_rate=5e-3, n_layers=1, size=3, entropy_weight=0.0):\n","    discrete = isinstance(env.action_space, gym.spaces.Discrete)\n","\n","    obs_dim = env.observation_space.shape[0]\n","    action_dim = env.action_space.n if discrete else env.action_space.shape[0]\n","\n","    obs_ph = tf.compat.v1.placeholder(shape=[None, obs_dim], name=\"obs_ph\", dtype=tf.float32)\n","    if discrete:\n","        action_ph = tf.compat.v1.placeholder(shape=[None], name=\"action_ph\", dtype=tf.int32)\n","    else:\n","        action_ph = tf.compat.v1.placeholder(shape=[None, action_dim], name=\"action_ph\", dtype=tf.float32)\n","\n","    advantage_ph = tf.compat.v1.placeholder(shape=[None], name=\"advantage_ph\", dtype=tf.float32)\n","\n","    if discrete:\n","        action_logits = encode_observation(\n","            obs_ph=obs_ph,\n","            output_size=action_dim,\n","            scope=\"action_logits\",\n","            n_layers=n_layers,\n","            size=size,\n","            activation=tf.nn.relu)\n","\n","        sampled_action = tf.compat.v1.squeeze(tf.compat.v1.multinomial(action_logits, 1), axis=[1])\n","        entropy = tf.compat.v1.reduce_sum(-tf.compat.v1.nn.softmax(action_logits) * tf.compat.v1.nn.log_softmax(action_logits), axis=-1)\n","        neg_log_probs = tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(labels=action_ph, logits=action_logits)\n","\n","    else:\n","        action_mean = encode_observation(\n","            obs_ph=obs_ph,\n","            output_size=action_dim,\n","            scope=\"action_mean\",\n","            n_layers=n_layers,\n","            size=size,\n","            activation=tf.nn.relu)\n","\n","        action_log_stdev = tf.compat.v1.get_variable(\"log_stdev\", shape=[action_dim])\n","        sampled_action = action_mean + tf.compat.v1.multiply(tf.exp(action_log_stdev),\n","                                                   tf.compat.v1.random_normal(tf.compat.v1.shape(action_mean)))\n","        distribution = tf.compat.v1.contrib.distributions.MultivariateNormalDiag(loc=action_mean,\n","                                                                       scale_diag=tf.exp(action_log_stdev))\n","        neg_log_probs = -distribution.log_prob(action_ph)\n","        entropy = distribution.entropy()\n","\n","    weighted_neg_log_probs = tf.compat.v1.multiply(neg_log_probs, advantage_ph)\n","    policy_loss = tf.compat.v1.reduce_mean(weighted_neg_log_probs - entropy * entropy_weight)\n","    policy_train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(policy_loss)\n","\n","    # compute critic loss\n","    value_prediction = tf.squeeze(encode_observation(\n","        obs_ph=obs_ph,\n","        output_size=1,\n","        scope=\"value_prediction\",\n","        n_layers=n_layers,\n","        size=size))\n","\n","    value_ph = tf.compat.v1.placeholder(shape=[None], dtype=tf.float32)\n","    critic_loss = tf.compat.v1.losses.mean_squared_error(predictions=value_prediction, labels=value_ph)\n","    critic_train_op = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(critic_loss)\n","\n","    return A2cAgent(obs_ph=obs_ph, action_ph=action_ph, advantage_ph=advantage_ph, value_ph=value_ph,\n","                    sampled_action=sampled_action,\n","                    critic_train_op=critic_train_op, value_prediction=value_prediction,\n","                    policy_loss=policy_loss, policy_train_op=policy_train_op)"],"execution_count":12,"outputs":[]},{"metadata":{"id":"MBOBD__UkVBD"},"cell_type":"markdown","source":["## Experiment\n","\n","Finally, we define an experiment for an agent and environment. This function takes as input an [environment](https://gym.openai.com/envs/#classic_control) and a set of parameters for the neural network. It returns a trained agent, which can be used for evaluation, and episode summaries.\n","\n"]},{"metadata":{"id":"u6ZaDrwwkWxy","executionInfo":{"status":"ok","timestamp":1652808683906,"user_tz":-120,"elapsed":244,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}}},"cell_type":"code","source":["def run_experiment(env_name='CartPole-v0',\n","                   n_iter=100,\n","                   num_ep_per_iter=34,\n","                   gamma=1.0,\n","                   max_episode_len=None,\n","                   learning_rate=5e-3,\n","                   entropy_weight=0.0,\n","                   enable_critic=False,\n","                   seed=0,\n","                   n_layers=1,\n","                   size=32\n","                   ):\n","    start = time.time()\n","\n","    # Set random seeds\n","    tf.compat.v1.set_random_seed(seed)\n","    np.random.seed(seed)\n","\n","    # Make gym environment\n","    env = gym.make(env_name)\n","    max_episode_len = max_episode_len or env.spec.max_episode_steps\n","\n","    # build neural nets\n","    agent = build_network(env=env, learning_rate=learning_rate, n_layers=n_layers, size=size,\n","                          entropy_weight=entropy_weight)\n","\n","    sess = tf.compat.v1.Session()\n","    sess.__enter__()\n","    tf.compat.v1.global_variables_initializer().run()\n","\n","    total_timesteps = 0\n","    summaries = []\n","    for itr in range(n_iter):\n","\n","        episodes = collect_n_episodes(num_ep=num_ep_per_iter, max_ep_len=max_episode_len, env=env, agent=agent,\n","                                      session=sess)\n","\n","        # [batch_size, obs_dim]\n","        obs_input = np.concatenate([e.observations for e in episodes])\n","        # [batch_size, action_dim]\n","        action_input = np.concatenate([e.actions for e in episodes])\n","\n","        def discount_rewards_to_go(rewards, gamma):\n","            res = []\n","            future_reward = 0\n","            for r in reversed(rewards):\n","                future_reward = future_reward * gamma + r\n","                res.append(future_reward)\n","            return res[::-1]\n","\n","        # [batch_size]\n","        state_action_values = np.concatenate([discount_rewards_to_go(e.rewards, gamma) for e in episodes])\n","\n","        if enable_critic:\n","            state_values = sess.run(agent.value_prediction, feed_dict={agent.obs_ph: obs_input})\n","            advantages = state_action_values - state_values\n","        else:\n","            advantages = state_action_values.copy()\n","\n","        advantages = (advantages - np.mean(advantages)) / np.std(advantages)\n","\n","        if enable_critic:\n","            sess.run(agent.critic_train_op,\n","                     feed_dict={agent.obs_ph: obs_input, agent.value_ph: state_action_values})\n","\n","        _, policy_loss_value = sess.run([agent.policy_train_op, agent.policy_loss],\n","                                        feed_dict={agent.obs_ph: obs_input, agent.action_ph: action_input,\n","                                                   agent.advantage_ph: advantages})\n","\n","        # Log diagnostics\n","        batch_size = np.sum([e.length for e in episodes])\n","        total_timesteps += batch_size\n","        time_elapsed = time.time() - start\n","        summary = compute_summary(episodes, time_elapsed, policy_loss_value, total_timesteps)\n","        \n","        print(summary)\n","        summaries.append(summary)\n","\n","    # return the trained agent, so we can use it for evaluation.\n","    # also return summary for plotting\n","    return agent, summaries, sess"],"execution_count":13,"outputs":[]},{"metadata":{"id":"D78hpbAG3Tof"},"cell_type":"markdown","source":["## Task 1: Train an agent on CartPole\n","\n","Run the following code cell to tune the agent on [CartPole](https://gym.openai.com/envs/CartPole-v1/), modifying the different parameters' values."]},{"metadata":{"id":"pGL94BclgrH4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652808738278,"user_tz":-120,"elapsed":48262,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}},"outputId":"2c1bbbaa-7165-43bb-b791-9ed44f73e520"},"cell_type":"code","source":["env_name = 'CartPole-v0' #@param [\"CartPole-v0\", \"MountainCarContinuous-v0\"]\n","num_iterations =100 #@param {type:\"slider\", min:1, max:300, step:1}\n","num_episodes_per_iteration = 6 #@param {type:\"slider\", min:1, max:300, step:1}\n","discount_factor =0.99 #@param {type:\"slider\", min:0.01, max:1, step:0.01}\n","max_episode_length = 154 #@param {type:\"slider\", min:0, max:500, step:1}\n","entropy_weight = 0.011 #@param {type:\"slider\", min:0.0, max:0.1, step:0.001}\n","learning_rate =5e-3 #@param\n","random_seed = 23 #@param\n","number_of_layers = 3 #@param {type:\"slider\", min:0, max:20, step:1}\n","number_of_neuron_per_layer = 32 #@param {type:\"slider\", min:1, max:100, step:1}\n","enable_critic = True #@param {type:\"boolean\"}\n","\n","with tf.compat.v1.Graph().as_default():\n","  run_experiment(env_name=env_name,\n","               n_iter=num_iterations,\n","               num_ep_per_iter=num_episodes_per_iteration,\n","               gamma=discount_factor,\n","               max_episode_len=max_episode_length,\n","               learning_rate=learning_rate,\n","               enable_critic=enable_critic,\n","               entropy_weight = entropy_weight,\n","               seed=random_seed,\n","               n_layers=number_of_layers,\n","               size=number_of_neuron_per_layer)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  \n"]},{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.random.categorical` instead.\n","Summary(time_elapsed=3.710810661315918, global_steps=103, mean_return=17.166666666666668, std_return=5.428832491634109, min_return=11.0, max_return=26.0, mean_episode_len=17.166666666666668, std_episode_len=5.428832491634109, policy_loss=-0.0103507)\n","Summary(time_elapsed=3.84132981300354, global_steps=239, mean_return=22.666666666666668, std_return=14.761059883656351, min_return=11.0, max_return=55.0, mean_episode_len=22.666666666666668, std_episode_len=14.761059883656351, policy_loss=-0.0052843294)\n","Summary(time_elapsed=4.007715225219727, global_steps=351, mean_return=18.666666666666668, std_return=7.272474743090476, min_return=11.0, max_return=33.0, mean_episode_len=18.666666666666668, std_episode_len=7.272474743090476, policy_loss=-0.00991423)\n","Summary(time_elapsed=4.250180721282959, global_steps=542, mean_return=31.833333333333332, std_return=17.752151669273474, min_return=13.0, max_return=66.0, mean_episode_len=31.833333333333332, std_episode_len=17.752151669273474, policy_loss=-0.0269969)\n","Summary(time_elapsed=4.392812490463257, global_steps=700, mean_return=26.333333333333332, std_return=15.92342788332825, min_return=12.0, max_return=59.0, mean_episode_len=26.333333333333332, std_episode_len=15.92342788332825, policy_loss=-0.03718769)\n","Summary(time_elapsed=4.488844394683838, global_steps=797, mean_return=16.166666666666668, std_return=4.94694069321861, min_return=11.0, max_return=24.0, mean_episode_len=16.166666666666668, std_episode_len=4.94694069321861, policy_loss=-0.019401835)\n","Summary(time_elapsed=4.61648416519165, global_steps=964, mean_return=27.833333333333332, std_return=10.269967002002598, min_return=14.0, max_return=46.0, mean_episode_len=27.833333333333332, std_episode_len=10.269967002002598, policy_loss=-0.017152991)\n","Summary(time_elapsed=4.779022693634033, global_steps=1121, mean_return=26.166666666666668, std_return=14.40389607787498, min_return=17.0, max_return=58.0, mean_episode_len=26.166666666666668, std_episode_len=14.40389607787498, policy_loss=-0.018407794)\n","Summary(time_elapsed=4.908706188201904, global_steps=1218, mean_return=16.166666666666668, std_return=3.975620147292188, min_return=11.0, max_return=24.0, mean_episode_len=16.166666666666668, std_episode_len=3.975620147292188, policy_loss=-0.009410855)\n","Summary(time_elapsed=5.068570375442505, global_steps=1372, mean_return=25.666666666666668, std_return=11.04033010778613, min_return=14.0, max_return=48.0, mean_episode_len=25.666666666666668, std_episode_len=11.04033010778613, policy_loss=-0.028179267)\n","Summary(time_elapsed=5.195857524871826, global_steps=1520, mean_return=24.666666666666668, std_return=13.337499349161705, min_return=12.0, max_return=53.0, mean_episode_len=24.666666666666668, std_episode_len=13.337499349161705, policy_loss=-0.032950293)\n","Summary(time_elapsed=5.508072853088379, global_steps=1850, mean_return=55.0, std_return=45.02591846185187, min_return=24.0, max_return=154.0, mean_episode_len=55.0, std_episode_len=45.02591846185187, policy_loss=-0.015603482)\n","Summary(time_elapsed=5.751051664352417, global_steps=2093, mean_return=40.5, std_return=28.47074521914264, min_return=13.0, max_return=95.0, mean_episode_len=40.5, std_episode_len=28.47074521914264, policy_loss=-0.035456605)\n","Summary(time_elapsed=5.973469257354736, global_steps=2331, mean_return=39.666666666666664, std_return=14.161763857498668, min_return=19.0, max_return=57.0, mean_episode_len=39.666666666666664, std_episode_len=14.161763857498668, policy_loss=-0.06939546)\n","Summary(time_elapsed=6.164935350418091, global_steps=2546, mean_return=35.833333333333336, std_return=18.487984085766502, min_return=15.0, max_return=73.0, mean_episode_len=35.833333333333336, std_episode_len=18.487984085766502, policy_loss=-0.014157894)\n","Summary(time_elapsed=6.449969053268433, global_steps=2857, mean_return=51.833333333333336, std_return=28.678776511947337, min_return=18.0, max_return=89.0, mean_episode_len=51.833333333333336, std_episode_len=28.678776511947337, policy_loss=-0.014337402)\n","Summary(time_elapsed=6.793860197067261, global_steps=3237, mean_return=63.333333333333336, std_return=33.53936327494738, min_return=21.0, max_return=122.0, mean_episode_len=63.333333333333336, std_episode_len=33.53936327494738, policy_loss=-0.02621061)\n","Summary(time_elapsed=7.094841957092285, global_steps=3566, mean_return=54.833333333333336, std_return=30.013422923011554, min_return=19.0, max_return=95.0, mean_episode_len=54.833333333333336, std_episode_len=30.013422923011554, policy_loss=-0.02395986)\n","Summary(time_elapsed=7.553042650222778, global_steps=4062, mean_return=82.66666666666667, std_return=36.61814971962522, min_return=47.0, max_return=154.0, mean_episode_len=82.66666666666667, std_episode_len=36.61814971962522, policy_loss=-0.021223852)\n","Summary(time_elapsed=8.088639259338379, global_steps=4664, mean_return=100.33333333333333, std_return=39.88176971443915, min_return=28.0, max_return=151.0, mean_episode_len=100.33333333333333, std_episode_len=39.88176971443915, policy_loss=-0.023222478)\n","Summary(time_elapsed=8.528114318847656, global_steps=5153, mean_return=81.5, std_return=35.344730866141845, min_return=37.0, max_return=148.0, mean_episode_len=81.5, std_episode_len=35.344730866141845, policy_loss=-0.011577127)\n","Summary(time_elapsed=9.05097484588623, global_steps=5693, mean_return=90.0, std_return=39.761790704142086, min_return=21.0, max_return=142.0, mean_episode_len=90.0, std_episode_len=39.761790704142086, policy_loss=-0.018702125)\n","Summary(time_elapsed=9.516394853591919, global_steps=6265, mean_return=95.33333333333333, std_return=25.77897506798041, min_return=54.0, max_return=125.0, mean_episode_len=95.33333333333333, std_episode_len=25.77897506798041, policy_loss=-0.030038804)\n","Summary(time_elapsed=9.991453886032104, global_steps=6881, mean_return=102.66666666666667, std_return=35.24990149710429, min_return=45.0, max_return=154.0, mean_episode_len=102.66666666666667, std_episode_len=35.24990149710429, policy_loss=-0.040000893)\n","Summary(time_elapsed=10.581317901611328, global_steps=7587, mean_return=117.66666666666667, std_return=29.52776471202805, min_return=63.0, max_return=149.0, mean_episode_len=117.66666666666667, std_episode_len=29.52776471202805, policy_loss=-0.018387452)\n","Summary(time_elapsed=11.162089347839355, global_steps=8273, mean_return=114.33333333333333, std_return=29.420325551488304, min_return=84.0, max_return=154.0, mean_episode_len=114.33333333333333, std_episode_len=29.420325551488304, policy_loss=-0.031154908)\n","Summary(time_elapsed=11.646126747131348, global_steps=8947, mean_return=112.33333333333333, std_return=26.874192494328497, min_return=76.0, max_return=150.0, mean_episode_len=112.33333333333333, std_episode_len=26.874192494328497, policy_loss=0.0112398)\n","Summary(time_elapsed=12.022389650344849, global_steps=9638, mean_return=115.16666666666667, std_return=21.074602935497715, min_return=94.0, max_return=154.0, mean_episode_len=115.16666666666667, std_episode_len=21.074602935497715, policy_loss=-0.00010719658)\n","Summary(time_elapsed=12.356452465057373, global_steps=10239, mean_return=100.16666666666667, std_return=20.859982955143778, min_return=66.0, max_return=135.0, mean_episode_len=100.16666666666667, std_episode_len=20.859982955143778, policy_loss=-0.013495813)\n","Summary(time_elapsed=12.727695226669312, global_steps=10895, mean_return=109.33333333333333, std_return=25.32236604707827, min_return=81.0, max_return=154.0, mean_episode_len=109.33333333333333, std_episode_len=25.32236604707827, policy_loss=-6.906143e-05)\n","Summary(time_elapsed=13.132477521896362, global_steps=11601, mean_return=117.66666666666667, std_return=14.761059883656353, min_return=88.0, max_return=137.0, mean_episode_len=117.66666666666667, std_episode_len=14.761059883656353, policy_loss=-0.015327324)\n","Summary(time_elapsed=13.559780836105347, global_steps=12340, mean_return=123.16666666666667, std_return=20.594632526192083, min_return=93.0, max_return=154.0, mean_episode_len=123.16666666666667, std_episode_len=20.594632526192083, policy_loss=-0.0070063556)\n","Summary(time_elapsed=14.073754787445068, global_steps=13258, mean_return=153.0, std_return=2.23606797749979, min_return=148.0, max_return=154.0, mean_episode_len=153.0, std_episode_len=2.23606797749979, policy_loss=-0.0023616913)\n","Summary(time_elapsed=14.612018346786499, global_steps=14168, mean_return=151.66666666666666, std_return=5.217491947499509, min_return=140.0, max_return=154.0, mean_episode_len=151.66666666666666, std_episode_len=5.217491947499509, policy_loss=-0.0066054408)\n","Summary(time_elapsed=15.136573791503906, global_steps=15092, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=0.005115348)\n","Summary(time_elapsed=15.610767841339111, global_steps=15934, mean_return=140.33333333333334, std_return=30.559595692497126, min_return=72.0, max_return=154.0, mean_episode_len=140.33333333333334, std_episode_len=30.559595692497126, policy_loss=-0.034149326)\n","Summary(time_elapsed=16.105178833007812, global_steps=16858, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=0.009145423)\n","Summary(time_elapsed=16.609127521514893, global_steps=17782, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.017670076)\n","Summary(time_elapsed=17.11633801460266, global_steps=18691, mean_return=151.5, std_return=3.8188130791298667, min_return=144.0, max_return=154.0, mean_episode_len=151.5, std_episode_len=3.8188130791298667, policy_loss=-0.040236544)\n","Summary(time_elapsed=17.496163368225098, global_steps=19399, mean_return=118.0, std_return=30.978487159102308, min_return=63.0, max_return=154.0, mean_episode_len=118.0, std_episode_len=30.978487159102308, policy_loss=-0.05414188)\n","Summary(time_elapsed=18.02681851387024, global_steps=20323, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.028065246)\n","Summary(time_elapsed=18.44973611831665, global_steps=21111, mean_return=131.33333333333334, std_return=26.662499674428297, min_return=75.0, max_return=154.0, mean_episode_len=131.33333333333334, std_episode_len=26.662499674428297, policy_loss=-0.047284283)\n","Summary(time_elapsed=18.92969846725464, global_steps=21931, mean_return=136.66666666666666, std_return=21.615323782497967, min_return=95.0, max_return=154.0, mean_episode_len=136.66666666666666, std_episode_len=21.615323782497967, policy_loss=-0.052409887)\n","Summary(time_elapsed=19.411356449127197, global_steps=22822, mean_return=148.5, std_return=12.298373876248844, min_return=121.0, max_return=154.0, mean_episode_len=148.5, std_episode_len=12.298373876248844, policy_loss=-0.047936186)\n","Summary(time_elapsed=19.88546633720398, global_steps=23687, mean_return=144.16666666666666, std_return=17.892425461319906, min_return=105.0, max_return=154.0, mean_episode_len=144.16666666666666, std_episode_len=17.892425461319906, policy_loss=-0.036419354)\n","Summary(time_elapsed=20.39586615562439, global_steps=24578, mean_return=148.5, std_return=10.226599304428298, min_return=126.0, max_return=154.0, mean_episode_len=148.5, std_episode_len=10.226599304428298, policy_loss=-0.04018766)\n","Summary(time_elapsed=20.899364948272705, global_steps=25501, mean_return=153.83333333333334, std_return=0.3726779962499649, min_return=153.0, max_return=154.0, mean_episode_len=153.83333333333334, std_episode_len=0.3726779962499649, policy_loss=-0.026413789)\n","Summary(time_elapsed=21.389716863632202, global_steps=26412, mean_return=151.83333333333334, std_return=4.844813951249544, min_return=141.0, max_return=154.0, mean_episode_len=151.83333333333334, std_episode_len=4.844813951249544, policy_loss=-0.0085199)\n","Summary(time_elapsed=21.86799192428589, global_steps=27294, mean_return=147.0, std_return=13.152946437965905, min_return=118.0, max_return=154.0, mean_episode_len=147.0, std_episode_len=13.152946437965905, policy_loss=-0.035012152)\n","Summary(time_elapsed=22.364766359329224, global_steps=28218, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.047027558)\n","Summary(time_elapsed=22.87383270263672, global_steps=29123, mean_return=150.83333333333334, std_return=7.080881928749334, min_return=135.0, max_return=154.0, mean_episode_len=150.83333333333334, std_episode_len=7.080881928749334, policy_loss=-0.026829936)\n","Summary(time_elapsed=23.369758367538452, global_steps=30003, mean_return=146.66666666666666, std_return=16.397831834998456, min_return=110.0, max_return=154.0, mean_episode_len=146.66666666666666, std_episode_len=16.397831834998456, policy_loss=-0.039112464)\n","Summary(time_elapsed=23.88988208770752, global_steps=30927, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.027117366)\n","Summary(time_elapsed=24.374030351638794, global_steps=31841, mean_return=152.33333333333334, std_return=3.726779962499649, min_return=144.0, max_return=154.0, mean_episode_len=152.33333333333334, std_episode_len=3.726779962499649, policy_loss=-0.051048145)\n","Summary(time_elapsed=24.813249111175537, global_steps=32621, mean_return=130.0, std_return=45.1811169996198, min_return=30.0, max_return=154.0, mean_episode_len=130.0, std_episode_len=45.1811169996198, policy_loss=-0.05142856)\n","Summary(time_elapsed=25.299240112304688, global_steps=33525, mean_return=150.66666666666666, std_return=5.527707983925667, min_return=139.0, max_return=154.0, mean_episode_len=150.66666666666666, std_episode_len=5.527707983925667, policy_loss=-0.059182633)\n","Summary(time_elapsed=25.793594121932983, global_steps=34435, mean_return=151.66666666666666, std_return=5.217491947499509, min_return=140.0, max_return=154.0, mean_episode_len=151.66666666666666, std_episode_len=5.217491947499509, policy_loss=-0.06328567)\n","Summary(time_elapsed=26.23674464225769, global_steps=35273, mean_return=139.66666666666666, std_return=16.29587541544042, min_return=110.0, max_return=154.0, mean_episode_len=139.66666666666666, std_episode_len=16.29587541544042, policy_loss=-0.019433718)\n","Summary(time_elapsed=26.75443696975708, global_steps=36197, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.03750197)\n","Summary(time_elapsed=27.253061771392822, global_steps=37121, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.023414008)\n","Summary(time_elapsed=27.78714632987976, global_steps=38045, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.007790592)\n","Summary(time_elapsed=28.310333013534546, global_steps=38969, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.04594361)\n","Summary(time_elapsed=28.83294677734375, global_steps=39893, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.03298659)\n","Summary(time_elapsed=29.34320878982544, global_steps=40790, mean_return=149.5, std_return=10.062305898749054, min_return=127.0, max_return=154.0, mean_episode_len=149.5, std_episode_len=10.062305898749054, policy_loss=-0.034903854)\n","Summary(time_elapsed=29.87899899482727, global_steps=41714, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.048993744)\n","Summary(time_elapsed=30.390641927719116, global_steps=42638, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.045680176)\n","Summary(time_elapsed=30.96299719810486, global_steps=43562, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.024736406)\n","Summary(time_elapsed=31.496225833892822, global_steps=44486, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=0.01943652)\n","Summary(time_elapsed=32.02231502532959, global_steps=45410, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.022374613)\n","Summary(time_elapsed=32.519163370132446, global_steps=46334, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.014304241)\n","Summary(time_elapsed=33.05295777320862, global_steps=47258, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.0331067)\n","Summary(time_elapsed=33.54501676559448, global_steps=48182, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.0006337514)\n","Summary(time_elapsed=34.08318114280701, global_steps=49106, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.010670825)\n","Summary(time_elapsed=34.57322430610657, global_steps=50030, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.028126733)\n","Summary(time_elapsed=35.10216021537781, global_steps=50954, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.0067771343)\n","Summary(time_elapsed=35.59681820869446, global_steps=51878, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.008653719)\n","Summary(time_elapsed=36.12293243408203, global_steps=52802, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.023927957)\n","Summary(time_elapsed=36.64434337615967, global_steps=53726, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.004322058)\n","Summary(time_elapsed=37.160009145736694, global_steps=54650, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.020915337)\n","Summary(time_elapsed=37.68754482269287, global_steps=55574, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.015465289)\n","Summary(time_elapsed=38.1966598033905, global_steps=56498, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-5.6130804e-05)\n","Summary(time_elapsed=38.6948299407959, global_steps=57422, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.042652685)\n","Summary(time_elapsed=39.20385265350342, global_steps=58346, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.013109523)\n","Summary(time_elapsed=39.707589864730835, global_steps=59270, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.005569955)\n","Summary(time_elapsed=40.217106342315674, global_steps=60194, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.0008722967)\n","Summary(time_elapsed=40.72380328178406, global_steps=61118, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=0.012331316)\n","Summary(time_elapsed=41.25032997131348, global_steps=62042, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.0045460146)\n","Summary(time_elapsed=41.73534870147705, global_steps=62966, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=0.00014195737)\n","Summary(time_elapsed=42.250200271606445, global_steps=63890, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.012752147)\n","Summary(time_elapsed=42.75779318809509, global_steps=64814, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.031200431)\n","Summary(time_elapsed=43.28344774246216, global_steps=65738, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.01602575)\n","Summary(time_elapsed=43.76550555229187, global_steps=66662, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.0022569022)\n","Summary(time_elapsed=44.29153752326965, global_steps=67586, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.028252633)\n","Summary(time_elapsed=44.790011405944824, global_steps=68510, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.004770357)\n","Summary(time_elapsed=45.30062937736511, global_steps=69434, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=0.0039789295)\n","Summary(time_elapsed=45.80572438240051, global_steps=70358, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.01246007)\n","Summary(time_elapsed=46.31612777709961, global_steps=71282, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.008961899)\n","Summary(time_elapsed=46.82067275047302, global_steps=72206, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=0.009138335)\n","Summary(time_elapsed=47.33400797843933, global_steps=73130, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=-0.006461938)\n","Summary(time_elapsed=47.8315110206604, global_steps=74054, mean_return=154.0, std_return=0.0, min_return=154.0, max_return=154.0, mean_episode_len=154.0, std_episode_len=0.0, policy_loss=0.0028331233)\n"]}]},{"metadata":{"id":"-188J6eOpHB0"},"cell_type":"markdown","source":["## Task 2: Train the agent on Atari\n","\n","Now train an agent on the Atari 2600 game [Pong](https://gym.openai.com/envs/Pong-ram-v0/).\n","\n","The following code will help with the visualization."]},{"cell_type":"code","source":["%%bash\n","\n","# install required system dependencies\n","apt-get install -y xvfb x11-utils\n","\n","# install required python dependencies (might need to install additional gym extras depending)\n","pip install gym[box2d]==0.17.* pyvirtualdisplay==0.2.* PyOpenGL==3.1.* PyOpenGL-accelerate==3.1.*"],"metadata":{"id":"1ra1a25UBnHE","executionInfo":{"status":"ok","timestamp":1652808910639,"user_tz":-120,"elapsed":44916,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}},"outputId":"4a65b77d-5c6b-4197-ee1c-4920a9a33527","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists...\n","Building dependency tree...\n","Reading state information...\n","The following packages were automatically installed and are no longer required:\n","  libnvidia-common-460 nsight-compute-2020.2.0\n","Use 'apt autoremove' to remove them.\n","The following additional packages will be installed:\n","  libxxf86dga1\n","Suggested packages:\n","  mesa-utils\n","The following NEW packages will be installed:\n","  libxxf86dga1 x11-utils xvfb\n","0 upgraded, 3 newly installed, 0 to remove and 67 not upgraded.\n","Need to get 993 kB of archives.\n","After this operation, 2,982 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n","Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n","Fetched 993 kB in 1s (932 kB/s)\n","Selecting previously unselected package libxxf86dga1:amd64.\r\n","(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 155203 files and directories currently installed.)\r\n","Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\r\n","Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n","Selecting previously unselected package x11-utils.\r\n","Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\r\n","Unpacking x11-utils (7.7+3build1) ...\r\n","Selecting previously unselected package xvfb.\r\n","Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\r\n","Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\r\n","Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\r\n","Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\r\n","Setting up x11-utils (7.7+3build1) ...\r\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n","Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\r\n","/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n","\r\n","Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Collecting pyvirtualdisplay==0.2.*\n","  Downloading PyVirtualDisplay-0.2.5-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.6)\n","Collecting PyOpenGL-accelerate==3.1.*\n","  Downloading PyOpenGL-accelerate-3.1.5.tar.gz (538 kB)\n","Collecting EasyProcess\n","  Downloading EasyProcess-1.1-py3-none-any.whl (8.7 kB)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.21.6)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.5.0)\n","Collecting box2d-py~=2.3.5\n","  Downloading box2d_py-2.3.8-cp37-cp37m-manylinux1_x86_64.whl (448 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[box2d]==0.17.*) (0.16.0)\n","Building wheels for collected packages: PyOpenGL-accelerate\n","  Building wheel for PyOpenGL-accelerate (setup.py): started\n","  Building wheel for PyOpenGL-accelerate (setup.py): finished with status 'done'\n","  Created wheel for PyOpenGL-accelerate: filename=PyOpenGL_accelerate-3.1.5-cp37-cp37m-linux_x86_64.whl size=1599465 sha256=1012a194108fb7557f508011f4392c6ab14fa0cf1840e9dce2b4586274b65c4d\n","  Stored in directory: /root/.cache/pip/wheels/1c/f5/6f/169afb3f2d476c5e807f8515b3c9bc9b819c3962316aa804eb\n","Successfully built PyOpenGL-accelerate\n","Installing collected packages: EasyProcess, box2d-py, pyvirtualdisplay, PyOpenGL-accelerate\n","Successfully installed EasyProcess-1.1 PyOpenGL-accelerate-3.1.5 box2d-py-2.3.8 pyvirtualdisplay-0.2.5\n"]}]},{"cell_type":"code","source":["import pyvirtualdisplay\n","\n","\n","_display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n","                                    size=(1400, 900))\n","_display.start()"],"metadata":{"id":"aZ4ue0hIBsfI","executionInfo":{"status":"ok","timestamp":1652808917863,"user_tz":-120,"elapsed":1181,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}},"outputId":"5786b68c-4a28-4a90-ca98-a7c29c479106","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["!echo $DISPLAY"],"metadata":{"id":"SJBqKGzKCMtY","executionInfo":{"status":"ok","timestamp":1652808920949,"user_tz":-120,"elapsed":256,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}},"outputId":"9f1a4bbd-21ef-44a4-868e-0d73c0bd8c74","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":[":1001\n"]}]},{"metadata":{"id":"gBzrEs0JkXQr","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":460},"executionInfo":{"status":"error","timestamp":1652808934797,"user_tz":-120,"elapsed":12093,"user":{"displayName":"Antonio Fdez.","userId":"07158694995347245788"}},"outputId":"f637e317-ee51-4fd2-fab6-d4609608949c"},"cell_type":"code","source":["env_name = 'CartPole-v0' \n","\n","with tf.compat.v1.Graph().as_default():\n","  agent, _, session = run_experiment(env_name=env_name,\n","               n_iter=1,\n","               num_ep_per_iter=32,\n","               gamma=0.99,\n","               max_episode_len=150,\n","               learning_rate=5e-3,\n","               enable_critic=False,\n","               entropy_weight = 0.012,\n","               seed=3,\n","               n_layers=10,\n","               size=30)\n","\n","  frames = collect_n_episodes(1, 200, gym.make(env_name), \n","                              agent, session, animate=True)\n","  \n","  display_frames_as_gif(frames)"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n","  return layer.apply(inputs)\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n","  \n"]},{"output_type":"stream","name":"stdout","text":["Summary(time_elapsed=5.763512372970581, global_steps=636, mean_return=19.875, std_return=7.532222447591415, min_return=11.0, max_return=39.0, mean_episode_len=19.875, std_episode_len=7.532222447591415, policy_loss=-0.007752733)\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-8a57da48a0ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                               agent, session, animate=True)\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mdisplay_frames_as_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-4ab42187491e>\u001b[0m in \u001b[0;36mdisplay_frames_as_gif\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuncAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'loop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36mdisplay_animation\u001b[0;34m(anim, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34m\"\"\"Display the animation with an IPython HTML object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[0;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[1;32m     74\u001b[0m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[1;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                                  default_mode=default_mode))\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                             \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                             \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;31m# Reconnect signal for first draw if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# Call run here now that all frame grabbing is done. All temp files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# are available to be assembled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mMovieWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will call clean-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/JSAnimation/html_writer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJS_INCLUDE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n\u001b[0;32m--> 323\u001b[0;31m                                              \u001b[0mNframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                                              \u001b[0mfill_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                                              \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'HTMLWriter' object has no attribute '_temp_names'"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"LhTzoWu3EJGt"},"execution_count":null,"outputs":[]}]}